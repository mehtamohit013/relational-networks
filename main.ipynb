{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torch.nn.utils import rnn\n",
    "\n",
    "import torchtext as tt\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data'\n",
    "log_dir = './LSTM_RN_logs'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "n_worker = 8\n",
    "\n",
    "input_dim = 13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_acc(y_true:torch.Tensor,y_pred:torch.Tensor,relations,k=1):\n",
    "    \n",
    "    y_pred_tpk = torch.topk(y_pred,k,dim=1)[1]\n",
    "    \n",
    "    unary_ovr = 0\n",
    "    unary_pos = 0\n",
    "    \n",
    "    binary_ovr = 0\n",
    "    binary_pos = 0\n",
    "\n",
    "    for i in range(0,len(y_pred_tpk)):\n",
    "        if(y_true[i] in y_pred_tpk[i]):\n",
    "            if relations[i]==1:  \n",
    "                unary_pos+=1\n",
    "            else:\n",
    "                binary_pos+=1\n",
    "        if relations[i]==1:\n",
    "            unary_ovr+=1\n",
    "        else:\n",
    "            binary_ovr+=1\n",
    "    \n",
    "    binary_acc = float(binary_pos)/float(binary_ovr)\n",
    "    unary_acc = float(unary_pos)/float(unary_ovr)\n",
    "    return binary_acc*100, unary_acc*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle(os.path.join(data_dir,'train_df.pkl'))\n",
    "test_df = pd.read_pickle(os.path.join(data_dir,'test_df.pkl'))\n",
    "\n",
    "n_train = len(train_df)\n",
    "n_test = len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tt.data.utils.get_tokenizer('basic_english')\n",
    "\n",
    "corpus = ''\n",
    "\n",
    "for i in train_df.index.values:\n",
    "    corpus+=' '+train_df.at[i,'Question']\n",
    "    corpus+=' '+str(train_df.at[i,'Answer'])\n",
    "\n",
    "corpus = corpus.lower()\n",
    "corpus_token = tokenizer(corpus)\n",
    "\n",
    "counter = Counter(corpus_token)\n",
    "sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "vocab = tt.vocab.Vocab(ordered_dict,specials=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class state_dataset(Dataset):\n",
    "    def __init__(self,df:pd.DataFrame,\n",
    "                vocab:tt.vocab.Vocab,\n",
    "                tokenizer,\n",
    "                input_dim = input_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.color_map = {\n",
    "            'red': 1,\n",
    "            'green': 2,\n",
    "            'blue': 3,\n",
    "            'orange':4,\n",
    "            'grey': 5,\n",
    "            'yellow':6\n",
    "        }\n",
    "        self.shape_map = {\n",
    "            'rectangle': 1,\n",
    "            'circle': 2\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = dict()\n",
    "\n",
    "        # State Description\n",
    "        state = {}\n",
    "        state['center'] = np.stack(self.df.at[index,'State']['center'].to_numpy(),axis=0).astype('float32')\n",
    "        \n",
    "        color = self.df.at[index,'State']['color'].values\n",
    "        state['color'] = np.array([self.color_map[i.lower()] for i in color]).astype('float32')\n",
    "\n",
    "        shape = self.df.at[index,'State']['shape'].values\n",
    "        state['shape'] = np.array([self.shape_map[i.lower()] for i in shape]).astype('float32')\n",
    "\n",
    "#         state['size'] = self.df.at[index,'State']['size'].values.astype('float32')\n",
    "        sample['state'] = np.concatenate((state['center'],state['color'][...,None],state['shape'][...,None]),axis=1)\n",
    "\n",
    "        # Tokenizing Question and Answer\n",
    "        ques = self.tokenizer(self.df.at[index,'Question'])\n",
    "        sample['ques'] = np.array([self.one_hot(self.vocab[i.lower()]) for i in ques]).astype('float32')\n",
    "        sample['lengths'] = sample['ques'].shape[0]\n",
    "        \n",
    "        sample['ques'] = np.concatenate([sample['ques'],\n",
    "                                   np.zeros((self.input_dim-sample['lengths'],self.vocab_size))],\n",
    "                                  axis=0).astype('float32')\n",
    "        \n",
    "        ans = [str(self.df.at[index,'Answer'])]\n",
    "        sample['ans'] = np.array([self.vocab[i.lower()] for i in ans]).astype('float32')\n",
    "\n",
    "        #  Additional Information\n",
    "        if self.df.at[index,'Relation'].lower() == 'unary':\n",
    "            sample['relations']=1\n",
    "        else:\n",
    "            sample['relations']=2\n",
    "\n",
    "#         sample['ques type'] = self.df.at[index,'Ques type']\n",
    "\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def one_hot(self,a:int):\n",
    "        tmp = np.zeros(self.vocab_size)\n",
    "        tmp[a] = 1\n",
    "        return tmp.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = state_dataset(\n",
    "    df=train_df,\n",
    "    vocab=vocab,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "test_dataset = state_dataset(\n",
    "    df=test_df,\n",
    "    vocab=vocab,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers = n_worker,\n",
    "    drop_last = True,\n",
    "    shuffle = True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    pin_memory=True,\n",
    "    num_workers = n_worker,\n",
    "    drop_last = True,\n",
    "    shuffle = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class lstm_RN(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size:int = len(vocab),\n",
    "                 batch_size:int = BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.hidden_dim = 128\n",
    "        self.lstm = nn.LSTM(input_size=vocab_size,\n",
    "                           hidden_size =128,\n",
    "                           batch_first=True)\n",
    "        \n",
    "        self.g_fc1 = nn.Linear((4)*2+128, 256)\n",
    "        \n",
    "        self.g_fc2 = nn.Linear(256, 256)\n",
    "        self.g_fc3 = nn.Linear(256, 256)\n",
    "        self.g_fc4 = nn.Linear(256, 256)\n",
    "\n",
    "        self.f_fc1 = nn.Linear(256, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, self.vocab_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,ques,state,lengths):\n",
    "        curr_device = ques.device\n",
    "        \n",
    "        h0 = torch.randn(1,self.batch_size,self.hidden_dim).to(curr_device)\n",
    "        c0 = torch.randn(1,self.batch_size,self.hidden_dim).to(curr_device)\n",
    "        \n",
    "        cpu = torch.device('cpu')\n",
    "        lengths = lengths.to(cpu)\n",
    "        ques = rnn.pack_padded_sequence(ques.to('cpu'),lengths,batch_first=True,enforce_sorted=False)\n",
    "        ques = ques.to(curr_device)\n",
    "        \n",
    "\n",
    "        ques,_ = self.lstm(ques,(h0,c0))\n",
    "        ques,lengths = rnn.pad_packed_sequence(ques, batch_first=True)\n",
    "        ques = ques[np.arange(ques.shape[0]), lengths - 1, :]\n",
    "        \n",
    "        '''\n",
    "        Making Pairs:\n",
    "        ques shape: [BATCH_SIZE,128]\n",
    "        state shape: [BATCH_SIZE,6,4]\n",
    "        6 objects with each object having 4 features\n",
    "        '''\n",
    "        \n",
    "        obj_cnt = state.shape[1]\n",
    "        \n",
    "#         From model.py line 172\n",
    "#         Add Questions Everywhere\n",
    "        qst = torch.unsqueeze(ques,1)\n",
    "        qst = qst.repeat(1,obj_cnt,1)\n",
    "        qst = torch.unsqueeze(qst,2)\n",
    "        \n",
    "#         Cast all pairs against one another\n",
    "        x_i = torch.unsqueeze(state,1)\n",
    "        x_i = x_i.repeat(1,obj_cnt,1,1)\n",
    "        \n",
    "        x_j = torch.unsqueeze(state,2)\n",
    "        x_j = torch.cat([x_j,qst],3)\n",
    "        x_j = x_j.repeat(1,1,obj_cnt,1)\n",
    "        \n",
    "        # Concat all together\n",
    "        x_full = torch.cat([x_i,x_j],3)\n",
    "        \n",
    "        # Reshape for passing through network\n",
    "        x_ = x_full.view(self.batch_size * obj_cnt * obj_cnt, x_full.shape[-1])\n",
    "        \n",
    "        \n",
    "        x_ = self.g_fc1(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc2(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc3(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        x_ = self.g_fc4(x_)\n",
    "        x_ = F.relu(x_)\n",
    "        \n",
    "        x_g = x_.view(self.batch_size,obj_cnt*obj_cnt, 256)\n",
    "        x_g = x_g.sum(1).squeeze()\n",
    "        \n",
    "        x_f = self.f_fc1(x_g)\n",
    "        x_f = F.relu(x_f)\n",
    "        \n",
    "        x = self.fc2(x_f)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_RN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lightning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_rn_trainer(pl.LightningModule):\n",
    "    def __init__(self,\n",
    "                learning_rate = 1e-3,\n",
    "                vocab_size = len(vocab),\n",
    "                batch_size = BATCH_SIZE):\n",
    "        super().__init__()\n",
    "        self.model = lstm_RN()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.nll_loss = nn.NLLLoss()\n",
    "        self.lr = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "       \n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.parameters(),self.lr)\n",
    "        return opt\n",
    "    \n",
    "    def forward(self,ques,state,lengths):\n",
    "        out = self.model(ques,state,lengths)\n",
    "        return out\n",
    "    \n",
    "    def training_step(self,batch,batch_idx):\n",
    "        ques = batch['ques']\n",
    "        state = batch['state']\n",
    "        lengths = batch['lengths']\n",
    "        label = batch['ans'][:,0].long()\n",
    "        \n",
    "        yhat = self(ques,state,lengths)\n",
    "        loss = self.nll_loss(yhat,label)\n",
    "        \n",
    "        self.log('Train Loss',loss)\n",
    "        \n",
    "        return {'loss':loss,'pred':yhat.cpu().detach(),\n",
    "                'label':label.cpu().detach(),\n",
    "                'relations': batch['relations'].cpu().detach()}\n",
    "    \n",
    "    def training_epoch_end(self,train_out):\n",
    "        len_out = len(train_out)\n",
    "        y_pred = torch.Tensor(len_out*self.batch_size,self.vocab_size)\n",
    "        y_true = torch.Tensor(len_out*self.batch_size)\n",
    "        relations = []\n",
    "        \n",
    "        for i in range(0,len_out):\n",
    "            y_pred[i*self.batch_size:(i+1)*self.batch_size,:] = train_out[i]['pred'] \n",
    "            y_true[i*self.batch_size:(i+1)*self.batch_size] = train_out[i]['label']\n",
    "            relations.append(train_out[i]['relations'])\n",
    "        \n",
    "        relations = torch.concat(relations).numpy()\n",
    "\n",
    "        # Calculating Avg loss\n",
    "        avg_loss = torch.stack([x['loss'] for x in train_out]).mean()\n",
    "\n",
    "        top1_bin,top1_una = top_k_acc(y_true,y_pred,relations,k=1)\n",
    "\n",
    "        self.logger.experiment.add_scalar('Loss-Train per epoch',avg_loss,self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Binary Train Accuracy',top1_bin,self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Unary Train Accuracy',top1_una,self.current_epoch)\n",
    "\n",
    "        print(f'Binary Train accuracy is {top1_bin}')\n",
    "        print(f'Unary Train accuracy is {top1_una}')\n",
    "        \n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        ques = batch['ques']\n",
    "        state = batch['state']\n",
    "        lengths = batch['lengths']\n",
    "        label = batch['ans'][:,0].long()\n",
    "        \n",
    "        yhat = self(ques,state,lengths)\n",
    "        \n",
    "        return [yhat.cpu().detach(),label.cpu().detach(),\n",
    "                batch['relations'].cpu().detach()]\n",
    "     \n",
    "    def validation_epoch_end(self,val_out):\n",
    "        len_out = len(val_out)\n",
    "        y_pred = torch.Tensor(len_out*self.batch_size,self.vocab_size)\n",
    "        y_true = torch.Tensor(len_out*self.batch_size)\n",
    "        relations = []\n",
    "        for i in range(0,len_out):\n",
    "            y_pred[i*self.batch_size:(i+1)*self.batch_size,:] = val_out[i][0]\n",
    "            y_true[i*self.batch_size:(i+1)*self.batch_size] = val_out[i][1]\n",
    "            relations.append(val_out[i][2])\n",
    "            \n",
    "        relations = torch.concat(relations).numpy()\n",
    "        top1_bin,top1_una = top_k_acc(y_true,y_pred,relations,k=1)\n",
    "\n",
    "        self.logger.experiment.add_scalar('Binary Validation Accuracy',top1_bin,self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Unary Validation Accuracy',top1_una,self.current_epoch)\n",
    "\n",
    "\n",
    "        print(f'Binary Validation accuracy is {top1_bin}')\n",
    "        print(f'Unary Validation accuracy is {top1_una}')\n",
    "    \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        ques = batch['ques']\n",
    "        state = batch['state']\n",
    "        lengths = batch['lengths']\n",
    "        label = batch['ans'][:,0].long()\n",
    "        \n",
    "        yhat = self(ques,state,lengths)\n",
    "        \n",
    "        return [yhat.cpu().detach(),label.cpu().detach(),\n",
    "                batch['relations'].cpu().detach()]\n",
    "     \n",
    "    def test_epoch_end(self,val_out):\n",
    "        len_out = len(val_out)\n",
    "        y_pred = torch.Tensor(len_out*self.batch_size,self.vocab_size)\n",
    "        y_true = torch.Tensor(len_out*self.batch_size)\n",
    "        relations = []\n",
    "        for i in range(0,len_out):\n",
    "            y_pred[i*self.batch_size:(i+1)*self.batch_size,:] = val_out[i][0]\n",
    "            y_true[i*self.batch_size:(i+1)*self.batch_size] = val_out[i][1]\n",
    "            relations.append(val_out[i][2])\n",
    "            \n",
    "        relations = torch.concat(relations).numpy()\n",
    "        top1_bin,top1_una = top_k_acc(y_true,y_pred,relations,k=1)\n",
    "\n",
    "        self.logger.experiment.add_scalar('Binary Test Accuracy',top1_bin,self.current_epoch)\n",
    "        self.logger.experiment.add_scalar('Unary Test Accuracy',top1_una,self.current_epoch)\n",
    "        \n",
    "        self.logger.experiment.add_hparams(\n",
    "            {\n",
    "                'LR' : self.lr,\n",
    "                \"Batch Size\": self.batch_size,\n",
    "                \"Vocab Size\": self.vocab_size,\n",
    "                'overall params' : sum(p.numel() for p in self.model.parameters())\n",
    "            }\n",
    "        ) \n",
    "\n",
    "        print(f'Binary Test accuracy is {top1_bin}')\n",
    "        print(f'Unary Test accuracy is {top1_una}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = TensorBoardLogger(log_dir, name=\"LSTM_RN\",log_graph=True,default_hp_metric=False)\n",
    "lstm_model = lstm_rn_trainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_pl_trainer = pl.Trainer(\n",
    "                    accelerator='gpu', devices=1,\n",
    "                    max_epochs = 40,\n",
    "                    logger = logger,\n",
    "                    deterministic = False,\n",
    "                    auto_lr_find = False\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_pl_trainer.tune(lstm_model,train_loader,test_loader)\n",
    "lstm_pl_trainer.fit(lstm_model,train_loader,test_loader)\n",
    "lstm_pl_trainer.test(gps_model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "notify_time": "5",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "352px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "79f4bea843ae1021b7128efbf08a36d8ce68ce05bacf587e2707a162a3751bb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
